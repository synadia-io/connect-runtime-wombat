model_version: '1'
label: snowflake_put
name: snowflake_put
status: preview
description: |-
  Sends messages to Snowflake stages and, optionally, calls Snowpipe to load this data into one or more tables.
    
    In order to use a different stage and / or Snowpipe for each message, you can use function interpolations as described
    [here](/docs/configuration/interpolation#bloblang-queries). When using batching, messages are grouped by the calculated
    stage and Snowpipe and are streamed to individual files in their corresponding stage and, optionally, a Snowpipe
    `insertFiles` REST API call will be made for each individual file.
    
    ### Credentials
    
    Two authentication mechanisms are supported:
    - User/password
    - Key Pair Authentication
    
    #### User/password
    
    This is a basic authentication mechanism which allows you to PUT data into a stage. However, it is not compatible with
    Snowpipe.
    
    #### Key Pair Authentication
    
    This authentication mechanism allows Snowpipe functionality, but it does require configuring an SSH Private Key
    beforehand. Please consult the [documentation](https://docs.snowflake.com/en/user-guide/key-pair-auth.html#configuring-key-pair-authentication)
    for details on how to set it up and assign the Public Key to your user.
    
    Note that the Snowflake documentation [used to suggest](https://twitter.com/felipehoffa/status/1560811785606684672)
    using this command:
    
    ```shell
    openssl genrsa 2048 | openssl pkcs8 -topk8 -inform PEM -out rsa_key.p8
    ```
    
    to generate an encrypted SSH private key. However, in this case, it uses an encryption algorithm called
    `pbeWithMD5AndDES-CBC`, which is part of the PKCS#5 v1.5 and is considered insecure. Due to this, Bento does not
    support it and, if you wish to use password-protected keys directly, you must use PKCS#5 v2.0 to encrypt them by using
    the following command (as the current Snowflake docs suggest):
    
    ```shell
    openssl genrsa 2048 | openssl pkcs8 -topk8 -v2 des3 -inform PEM -out rsa_key.p8
    ```
    
    If you have an existing key encrypted with PKCS#5 v1.5, you can re-encrypt it with PKCS#5 v2.0 using this command:
    
    ```shell
    openssl pkcs8 -in rsa_key_original.p8 -topk8 -v2 des3 -out rsa_key.p8
    ```
    
    Please consult [this](https://linux.die.net/man/1/pkcs8) pkcs8 command documentation for details on PKCS#5 algorithms.
    
    ### Batching
    
    It's common to want to upload messages to Snowflake as batched archives. The easiest way to do this is to batch your
    messages at the output level and join the batch of messages with an
    [`archive`](/docs/components/processors/archive) and/or [`compress`](/docs/components/processors/compress)
    processor.
    
    For the optimal batch size, please consult the Snowflake [documentation](https://docs.snowflake.com/en/user-guide/data-load-considerations-prepare.html).
    
    ### Snowpipe
    
    Given a table called `WOMBAT_TBL` with one column of type `variant`:
    
    ```sql
    CREATE OR REPLACE TABLE WOMBAT_DB.PUBLIC.WOMBAT_TBL(RECORD variant)
    ```
    
    and the following `WOMBAT_PIPE` Snowpipe:
    
    ```sql
    CREATE OR REPLACE PIPE WOMBAT_DB.PUBLIC.WOMBAT_PIPE AUTO_INGEST = FALSE AS COPY INTO WOMBAT_DB.PUBLIC.WOMBAT_TBL FROM (SELECT * FROM @%WOMBAT_TBL) FILE_FORMAT = (TYPE = JSON COMPRESSION = AUTO)
    ```
    
    you can configure Bento to use the implicit table stage `@%WOMBAT_TBL` as the `stage` and
    `WOMBAT_PIPE` as the `snowpipe`. In this case, you must set `compression` to `AUTO` and, if
    using message batching, you'll need to configure an [`archive`](/docs/components/processors/archive) processor
    with the `concatenate` format. Since the `compression` is set to `AUTO`, the
    [gosnowflake](https://github.com/snowflakedb/gosnowflake) client library will compress the messages automatically so you
    don't need to add a [`compress`](/docs/components/processors/compress) processor for message batches.
    
    If you add `STRIP_OUTER_ARRAY = TRUE` in your Snowpipe `FILE_FORMAT`
    definition, then you must use `json_array` instead of `concatenate` as the archive processor format.
    
    Note: Only Snowpipes with `FILE_FORMAT` `TYPE` `JSON` are currently supported.
    
    ### Snowpipe Troubleshooting
    
    Snowpipe [provides](https://docs.snowflake.com/en/user-guide/data-load-snowpipe-rest-apis.html) the `insertReport`
    and `loadHistoryScan` REST API endpoints which can be used to get information about recent Snowpipe calls. In
    order to query them, you'll first need to generate a valid JWT token for your Snowflake account. There are two methods
    for doing so:
    - Using the `snowsql` [utility](https://docs.snowflake.com/en/user-guide/snowsql.html):
    
    ```shell
    snowsql --private-key-path rsa_key.p8 --generate-jwt -a <account> -u <user>
    ```
    
    - Using the Python `sql-api-generate-jwt` [utility](https://docs.snowflake.com/en/developer-guide/sql-api/authenticating.html#generating-a-jwt-in-python):
    
    ```shell
    python3 sql-api-generate-jwt.py --private_key_file_path=rsa_key.p8 --account=<account> --user=<user>
    ```
    
    Once you successfully generate a JWT token and store it into the `JWT_TOKEN` environment variable, then you can,
    for example, query the `insertReport` endpoint using `curl`:
    
    ```shell
    curl -H "Authorization: Bearer ${JWT_TOKEN}" "https://<account>.snowflakecomputing.com/v1/data/pipes/<database>.<schema>.<snowpipe>/insertReport"
    ```
    
    If you need to pass in a valid `requestId` to any of these Snowpipe REST API endpoints, you can set a
    [uuid_v4()](/bloblang/functions#uuid_v4) string in a metadata field called
    `request_id`, log it via the [`log`](/reference/components/processors/log) processor and
    then configure `request_id: ${ @request_id }` ). Alternatively, you can enable debug logging as described
    [here](/reference/configuration/logger) and Bento will print the Request IDs that it sends to Snowpipe.
    
    ### General Troubleshooting
    
    The underlying [`gosnowflake` driver](https://github.com/snowflakedb/gosnowflake) requires write access to
    the default directory to use for temporary files. Please consult the [`os.TempDir`](https://pkg.go.dev/os#TempDir)
    docs for details on how to change this directory via environment variables.
    
    A silent failure can occur due to [this issue](https://github.com/snowflakedb/gosnowflake/issues/701), where the
    underlying [`gosnowflake` driver](https://github.com/snowflakedb/gosnowflake) doesn't return an error and doesn't
    log a failure if it can't figure out the current username. One way to trigger this behaviour is by running Bento in a
    Docker container with a non-existent user ID (such as `--user 1000:1000`).
    
    
    # Performance
    
    This output benefits from sending multiple messages in flight in parallel for improved performance. You can tune the max number of in flight messages (or message batches) with the field `max_in_flight`.
    
    This output benefits from sending messages as a batch for improved performance. Batches can be formed at both the input and output level. You can find out more xref:configuration:batching.adoc[in this doc].
fields:
  - path: account
    name: account
    label: account
    kind: scalar
    type: string
    optional: false
    description: |-
      Account name, which is the same as the Account Identifier
        as described [here](https://docs.snowflake.com/en/user-guide/admin-account-identifier.html#where-are-account-identifiers-used).
        However, when using an [Account Locator](https://docs.snowflake.com/en/user-guide/admin-account-identifier.html#using-an-account-locator-as-an-identifier),
        the Account Identifier is formatted as `<account_locator>.<region_id>.<cloud>` and this field needs to be
        populated using the `<account_locator>` part.
  - path: region
    name: region
    label: region
    kind: scalar
    type: string
    optional: false
    examples:
      - us-west-2
    description: |-
      Optional region field which needs to be populated when using
        an [Account Locator](https://docs.snowflake.com/en/user-guide/admin-account-identifier.html#using-an-account-locator-as-an-identifier)
        and it must be set to the `<region_id>` part of the Account Identifier
        (`<account_locator>.<region_id>.<cloud>`).
  - path: cloud
    name: cloud
    label: cloud
    kind: scalar
    type: string
    optional: false
    examples:
      - aws
      - gcp
      - azure
    description: |-
      Optional cloud platform field which needs to be populated
        when using an [Account Locator](https://docs.snowflake.com/en/user-guide/admin-account-identifier.html#using-an-account-locator-as-an-identifier)
        and it must be set to the `<cloud>` part of the Account Identifier
        (`<account_locator>.<region_id>.<cloud>`).
  - path: user
    name: user
    label: user
    kind: scalar
    type: string
    optional: false
    description: |-
      Username.
  - path: password
    name: password
    label: password
    kind: scalar
    type: string
    optional: false
    description: |-
      An optional password.
    secret: true
  - path: private_key_file
    name: private_key_file
    label: private_key_file
    kind: scalar
    type: string
    optional: false
    description: |-
      The path to a file containing the private SSH key.
  - path: private_key_pass
    name: private_key_pass
    label: private_key_pass
    kind: scalar
    type: string
    optional: false
    description: |-
      An optional private SSH key passphrase.
    secret: true
  - path: role
    name: role
    label: role
    kind: scalar
    type: string
    optional: false
    description: |-
      Role.
  - path: database
    name: database
    label: database
    kind: scalar
    type: string
    optional: false
    description: |-
      Database.
  - path: warehouse
    name: warehouse
    label: warehouse
    kind: scalar
    type: string
    optional: false
    description: |-
      Warehouse.
  - path: schema
    name: schema
    label: schema
    kind: scalar
    type: string
    optional: false
    description: |-
      Schema.
  - path: stage
    name: stage
    label: stage
    kind: scalar
    type: string
    optional: false
    description: |-
      Stage name. Use either one of the
        [supported](https://docs.snowflake.com/en/user-guide/data-load-local-file-system-create-stage.html) stage types.
  - path: path
    name: path
    label: path
    kind: scalar
    type: string
    default: '""'
    optional: true
    description: |-
      Stage path.
  - path: file_name
    name: file_name
    label: file_name
    kind: scalar
    type: string
    default: '""'
    optional: true
    description: |-
      Stage file name. Will be equal to the Request ID if not set or empty.
  - path: file_extension
    name: file_extension
    label: file_extension
    kind: scalar
    type: string
    default: '""'
    optional: true
    examples:
      - csv
      - parquet
    description: |-
      Stage file extension. Will be derived from the configured `compression` if not set or empty.
  - path: upload_parallel_threads
    name: upload_parallel_threads
    label: upload_parallel_threads
    kind: scalar
    type: int
    default: '4'
    optional: true
    description: |-
      Specifies the number of threads to use for uploading files.
  - path: compression
    name: compression
    label: compression
    kind: scalar
    type: string
    default: '"AUTO"'
    optional: true
    description: |-
      Compression type.
  - path: request_id
    name: request_id
    label: request_id
    kind: scalar
    type: string
    default: '""'
    optional: true
    description: |-
      Request ID. Will be assigned a random UUID (v4) string if not set or empty.
  - path: snowpipe
    name: snowpipe
    label: snowpipe
    kind: scalar
    type: string
    optional: false
    description: |-
      An optional Snowpipe name. Use the `<snowpipe>` part from `<database>.<schema>.<snowpipe>`.
  - path: client_session_keep_alive
    name: client_session_keep_alive
    label: client_session_keep_alive
    kind: scalar
    type: bool
    default: 'false'
    optional: true
    description: |-
      Enable Snowflake keepalive mechanism to prevent the client session from expiring after 4 hours (error 390114).
  - path: batching
    name: batching
    label: batching
    kind: scalar
    type: object
    optional: false
    examples:
      - byte_size: 5000
        count: 0
        period: 1s
      - count: 10
        period: 1s
      - check: this.contains("END BATCH")
        count: 0
        period: 1m
    description: |-
      Allows you to configure a [https://wombat.dev/pipelines/learn/batching/#batch-policy](batching policy).
    fields:
      - path: batching.count
        name: count
        label: count
        kind: scalar
        type: int
        default: '0'
        optional: true
        description: |-
          A number of messages at which the batch should be flushed. If `0` disables count based batching.
      - path: batching.byte_size
        name: byte_size
        label: byte_size
        kind: scalar
        type: int
        default: '0'
        optional: true
        description: |-
          An amount of bytes at which the batch should be flushed. If `0` disables size based batching.
      - path: batching.period
        name: period
        label: period
        kind: scalar
        type: string
        default: '""'
        optional: true
        examples:
          - 1s
          - 1m
          - 500ms
        description: |-
          A period in which an incomplete batch should be flushed regardless of its size.
      - path: batching.check
        name: check
        label: check
        kind: scalar
        type: string
        default: '""'
        optional: true
        examples:
          - this.type == "end_of_transaction"
        description: |-
          A [https://wombat.dev/pipelines/learn/interpolation/#bloblang-queries](Bloblang query) that should return a boolean value indicating whether a message should end a batch.
  - path: max_in_flight
    name: max_in_flight
    label: max_in_flight
    kind: scalar
    type: int
    default: '1'
    optional: true
    description: |-
      The maximum number of parallel message batches to have in flight at any given time.
